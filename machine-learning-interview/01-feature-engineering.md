1. #### Why is it important to do the normalization? (特征归一化)

   对数值类型的特征做归一化可以将所有的特征都统一到一个大致相同的数值区间内。最常用的方法主要有以下两种。 

   （1）线性函数归一化**（Min-Max Scaling）**。它对原始数据进行线性变换，使结果映射到[0, 1]的范围，实现对原始数据的等比缩放。归一化公式如下

   [![img](https://camo.githubusercontent.com/52661e73aa105dd15274612c67d166c90d4867b8/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f585f2537426e6f726d25374425323025334425323025354366726163253742582d585f2537426d696e253744253744253742585f2537426d61782537442532302d253230585f2537426d696e253744253744)](https://camo.githubusercontent.com/52661e73aa105dd15274612c67d166c90d4867b8/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f585f2537426e6f726d25374425323025334425323025354366726163253742582d585f2537426d696e253744253744253742585f2537426d61782537442532302d253230585f2537426d696e253744253744)

   其中X为原始数据，Xmax、Xmin分别为数据最大值和最小值。 

   （2）零均值归一化（**Z-Score Normalization**）。它会将原始数据映射到均值为0、标准差为1的分布上。具体来说，假设原始特征的均值为μ、标准差为σ，那么归一化公式定义为

   [![img](https://camo.githubusercontent.com/6bd33f6003d2f308b73b1c33a1d7341294a74e6f/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f7a25323025334425323025354366726163253742782d2535436d752537442537422535437369676d61253744)](https://camo.githubusercontent.com/6bd33f6003d2f308b73b1c33a1d7341294a74e6f/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f7a25323025334425323025354366726163253742782d2535436d752537442537422535437369676d61253744)

   当然，数据归一化并不是万能的。在实际应用中，通过梯度下降法求解的模型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模型。但**对于决策树模型则并不适用**，以C4.5为例，决策树在进行节点分裂时主要依据数据集D关于特征x的信息增益比（详见第3章第3节），而信息增益比跟特征是否经过归一化是无关的，因为**归一化并不会改变样本在特征x上的信息增益。**

   

   假设有两种数值型特征，x1的取值范围为  [0,  10]，x2的取值范围为[0, 3]，于是可以构造一个目标函数符合图1.1（a）中的等值图。在学习速率相同的情况下，x1的更新速度会大于x2，需要较多的迭代才能找到最优解。如果将x1和x2归一化到相同的数值区间后，优化目标的等值图会变成图1.1（b）中的圆形，x1和x2的更新速度变得更为一致，容易更快地通过梯度下降找到最优解。

   ![](http://images.iterate.site/blog/image/20190322/y5vblKb9PTve.png?imageslim)

2. #### When doing the data preprocessing, how to deal with the catagorical features?

   - 序号编码 **Ordinal Encoding** 序号编码通常用于处理类别间**具有大小关系的数据**。例如成绩，可以分为低、中、高三档，并且存在`高>中>低` 的排序关系。序号编码会按照大小关系对类别型特征赋予一个数值ID，例如高表示为3、中表示为2、低表示为1，转换后依然保留了大小关系。

   - 独热编码 **One-hot Encoding** 独热编码通常用于处理类别间**不具有大小关系的特征**。例如血型，一共有4个取值（A型血、B型血、AB型血、O型血），独热编码会把血型变成一个4d  稀疏向量，A型血表示为（1, 0, 0, 0），B型血表示为（0, 1, 0, 0），AB型表示为（0, 0,1, 0），O型血表示为（0,  0, 0, 1）。对于类别取值较多的情况下使用独热编码需要注意以下问题: 

     （1）使用 **sparse matrix 来节省空间**。在 one-hot  下，特征向量只有某一维取值为1，其他位置均为0。因此可以利用向量的稀疏表示有效地节省空间，并且目前大部分的算法均接受稀疏向量形式的输入。 

     （2）配合特征选择来降低维度。高维度特征会带来几方面的问题。一是在 <u>KNN  中，高维空间下两点之间的距离很难得到有效的衡量</u>；二是在<u>逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合问题；</u>三是通常<u>只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度。</u>

   - 二进制编码 **Binary Encoding** 二进制编码主要分为两步，先用序号编码给每个类别赋予一个类别ID，然后将类别ID对应的二进制编码作为结果。以A、B、AB、O血型为例，A型血的ID为1，二进制表示为001；B型血的ID为2，二进制表示为 010；以此类推可以得到AB型血和O型血的二进制表示。可以看出，二进制编码本质上是利用二进制对ID进行哈希映射，最终得到0/1特征向量，且维数少于 one-hot，节省了存储空间。

3. #### What is feature crosses? How to deal with high dimentional feature crosses?

   为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。

   We could use decision tree to conduct feature crosses.

   给定原始输入该如何有效地构造决策树呢？可以采用梯度提升决策树，该方法的思想是每次都在之前构建的决策树的残差上构建下一棵决策树。

4. #### Give some examples of text models, what are the pros and cons?

   - **bag of words and N-gram** 最基础的文本表示模型是词袋模型。顾名思义，就是将每篇文章看成一袋子词，并忽略每个词出现的顺序。具体地说，就是将整段文本以词为单位切分开，然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对应的权重则反映了这个词在原文章中的重要程度。常用TF-IDF来计算权重，公式为 

      `TF-IDF(t,d)=TF(t,d)×IDF(t) `

     其中TF(t,d)为单词t在文档d中出现的频率，IDF(t)是逆文档频率，用来衡量单词t对表达语义所起的重要性，表示为

     [![img](https://camo.githubusercontent.com/17b86f1a840e5cc58d1e61e56839382511cb47a4/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f4944462532387425323925323025334425323025354366726163253742746f74616c2535432533416e756d2535432533416f6625323025354325334161727469636c65732532302537442537426c6f6725323861727469636c657325354325334125323074686174253543253341636f6e7461696e73253543253341742532302b31253239253744)](https://camo.githubusercontent.com/17b86f1a840e5cc58d1e61e56839382511cb47a4/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f4944462532387425323925323025334425323025354366726163253742746f74616c2535432533416e756d2535432533416f6625323025354325334161727469636c65732532302537442537426c6f6725323861727469636c657325354325334125323074686174253543253341636f6e7461696e73253543253341742532302b31253239253744)

     直观的解释是，<u>如果一个单词在非常多的文章里面都出现，那么它可能是一个比较通用的词汇，对于区分某篇文章特殊语义的贡献较小，因此对权重做一定惩罚。</u> 将文章进行单词级别的划分有时候并不是一种好的做法，比如英文中的 natural language  processing 一词，如果将natural，language，processing 这3个词拆分开来，所表达的含义与三个词连续出现时大相径庭。通常，可以将连续出现的n个词（n≤N）组成的词组（N-gram）也作为一个单独的特征放到向量表示中去，构成N-gram模型。另外，同一个词可能有多种词性变化，却具有相似的含义。在实际应用中，一般会对单词进行词干抽取（Word  Stemming）处理，即将不同词性的单词统一成为同一词干的形式。

   - **topic modeling** 主题模型用于从文本库中发现有代表性的主题（得到每个主题上面词的分布特性），并且能够计算出每篇文章的主题分布

   - **word embedding and deep learning** 词嵌入是一类将词向量化的模型的统称，**核心思想是将每个词都映射成低维空间**（通常K=50～300维）上的一个稠密向量（Dense   Vector）。K维空间的每一维也可以看作一个隐含的主题，只不过不像主题模型中的主题那样直观。由于词嵌入将每个词映射成一个K维的向量，如果一篇文档有N个词，就可以用一个N×K维的矩阵来表示这篇文档，但是这样的表示过于底层。在实际应用中，如果仅仅把这个矩阵作为原文本的表示特征输入到机器学习模型中，通常很难得到令人满意的结果。因此，还需要在此基础之上加工出更高层的特征。在传统的浅层机器学习模型中，一个好的特征工程往往可以带来算法效果的显著提升。而深度学习模型正好为我们提供了一种自动地进行特征工程的方式，模型中的每个隐层都可以认为对应着不同抽象层次的特征。从这个角度来讲，深度学习模型能够打败浅层模型也就顺理成章了。卷积神经网络和循环神经网络的结构在文本表示中取得了很好的效果，主要是由于它们能够更好地对文本进行建模，抽取出一些高层的语义特征。与全连接的网络结构相比，卷积神经网络和循环神经网络一方面很好地抓住了文本的特性，另一方面又减少了网络中待学习的参数，提高了训练速度，并且降低了过拟合的风险。

5. #### How does Word2Vec work? what's the relationship with LDA?

   Word2Vec is the most common word embedding model, which is a <u>Shallow</u> neural network. It has two kinds of network structures, Continues bag of words and Skip-gram.

   CBOW的目标是根据上下文出现的词语来预测当前词的生成概率，如图1.3 (a) 所示; 而Skip-gram是根据当前词来预测上下文中各词的生成概率，如图1.3 (b)所示。

   ![](https://zdkswd.github.io/img/media/word2vec/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-03-26%20%E4%B8%8B%E5%8D%884.54.01.png)

   Each word in the input layer is represented by one-hot labels, which means each word is a N-Dimentional vector, and N is the total number of the words. 在向量中，每个词都将与之对应的维度置为1，其余维度的值均设为0。

   In hidden layers，K 个 hidden units 的取值可以由N维 input vector 以及连接输入和隐含单元之间的N×K维权重矩阵计算得到。在CBOW中，还需要将各个输入词所计算出的隐含单元求和。

   同理，输出层向量的值可以通过隐含层向量（K维），以及连接隐含层和输出层之间的K×N维权重矩阵计算得到。输出层也是一个N维向量，每维与词汇表中的一个单词相对应。最后，对输出层向量应用 **Softmax activation function**，可以计算出每个单词的生成概率。

   接下来的任务就是训练神经网络的权重，使得语料库中所有单词的整体生成概率最大化。从输入层到隐含层需要一个维度为N×K的权重矩阵，从隐含层到输出层又需要一个维度为K×N的权重矩阵，学习权重可以用反向传播算法实现，每次迭代时将权重沿梯度更优的方向进行一小步更新。但是由于Softmax激活函数中存在归一化项的缘故，推导出来的迭代公式需要对词汇表中的所有单词进行遍历，使得每次迭代过程非常缓慢，由此产生了Hierarchical Softmax和 Negative Sampling 两种改进方法，可以参考Word2Vec的原论文。训练得到维度为N×K和K×N的两个权重矩阵之后，可以选择其中一个作为N个词的K维向量表示。



​		**Word2Vec vs LDA?**

- LDA是利用文档中单词的共现关系来对单词 cluster by topics，也可以理解为对“文档-单词”矩阵进行分解，得到“文档-主题”和“主题-单词”两个概率分布。而 Word2Vec 其实是对“上下文-单词”矩阵进行学习，其中上下文由周围的几个单词组成，由此得到的词向量表示更多地融入了上下文共现的特征。也就是说, **如果两个单词所对应的 Word2Vec 向量相似度较高，那么它们很可能经常在同样的上下文中出现。**需要说明的是，上述分析的是LDA与Word2Vec的不同，不应该作为主题模型和词嵌入两类方法的主要差异。Topic modeling 通过一定的结构调整可以基于“上下文-单词”矩阵进行主题推理。同样地，词嵌入方法也可以根据“文档-单词”矩阵学习出词的隐含向量表示。主题模型和词嵌入两类方法最大的不同其实在于**模型本身**，topic modeling 是一种基于**概率图模型的生成式模型**，其似然函数可以写成若干条件概率连乘的形式，其中包括需要推测的隐含变量（即主题）；而**词嵌入模型一般表达为神经网络的形式**，似然函数定义在网络的输出之上，需要通过学习网络的权重以得到单词的 dense matrix 表示.

7. #### In image classification tasks, what kind of problems would insuffient training data bring? How to deal with that of problems?

   - 迁移学习（Transfer  Learning），生成对抗网络，图像处理，上采样技术，数据扩充.

   - 一个模型所能提供的信息一般来源于两个方面，一是**训练数据中蕴含的信息**；二是在模型的形成过程中（包括构造、学习、推理等），人们提供的**先验信息**。当训练数据不足时，说明模型从原始数据中获取的信息比较少，这种情况下要想保证模型的效果，就需要更多先验信息。先验信息可以作用在模型上，例如让模型采用特定的内在结构、条件假设或添加其他一些约束条件；先验信息也可以直接施加在数据集上，即根据特定的先验假设去调整、变换或扩展训练数据，让其展现出更多的、更有用的信息，以利于后续模型的训练和学习。具体到图像分类任务上，训练数据不足带来的问题主要表现在**过拟合方面**，即模型在训练样本上的效果可能不错，但在测试集上的泛化效果不佳。根据上述讨论，对应的处理方法大致也可以分两类，一是基于**模型**的方法，主要是采用**降低过拟合风险的措施**，包括**简化模型（如将非线性模型简化为线性模型）、添加约束项以缩小假设空间（如L1/L2正则项）、集成学习、Dropout超参数**等；二是基**于数据的**方法，主要通过数据扩充**（Data    Augmentation）**，即根据一些先验知识，在保持特定信息的前提下，对原始数据进行适当变换以达到扩充数据集的效果。具体到图像分类任务中，在保持图像类别不变的前提下，可以对训练集中的每幅图像进行以下变换。

     （1）一定程度内的随机旋转、平移、缩放、裁剪、填充、左右翻转等，这些变换对应着同一个目标在不同角度的观察结果。

     （2）对图像中的像素添加噪声扰动，比如椒盐噪声、高斯白噪声等。

     （3）颜色变换。例如，在图像的RGB颜色空间上进行主成分分析，得到3个主成分的特征向量p1,p2,p3及其对应的特征值λ1,λ2,λ3，然后在每个像素的RGB值上添加增量[p1,p2,p3]•[α1λ1,α2λ2,α3λ3]T，其中α1,α2,α3是均值为0、方差较小的高斯分布随机数。

     （4）改变图像的亮度、清晰度、对比度、锐度等。

     ![](https://oe9nbfytu.qnssl.com/c/e783f792a07c7b35bbf5090431ca3f64)

     [图像数据不足时，你可以试试数据扩充](https://toutiao.io/posts/i0ds5z/preview)

     

   - 除了直接在图像空间进行变换，还可以先对图像进行特征提取，然后在图像的特征空间内进行变换，利用一些通用的数据扩充或上采样技术，例如**SMOTE**（Synthetic Minority Over-sampling Technique）算法。抛开上述这些启发式的变换方法，使用生成模型也可以合成一些新样本，例如当今非常流行的生成式对抗网络模型。此外，借助已有的其他模型或数据来进行迁移学习在深度学习中也十分常见。例如，对于大部分图像分类任务，并不需要从头开始训练模型，而是借用一个在大规模数据集上预训练好的通用模型，并在针对目标任务的小数据集上进行微调（fine-tune），这种微调操作就可以看成是一种简单的迁移学习。

7. #### Why vector representation is much faster?

- 举例机器学习中最常见的线性回归拟合函数y=wx+b，在实际的问题场景中，x的维度很可能是成百上千个，如果使用传统的方法，将会面对求解成百上千个方程的巨大苦难，这是很难实现的。但是如果使用向量的表示方式，将传统的代数形式映射到向量空间来进行解决，则可以的用矩阵的形式表示出成百上千个维度，  同时也可以快速的进行求解. 

