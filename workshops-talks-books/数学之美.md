### [《数学之美》发布会吴军精彩演讲"怎样才能不山寨"](https://www.youtube.com/watch?v=x0zTN8aSeYs&t=104s)

整个信息论的基础就是数学。

写一个程序，如果是一个山寨的实现方法，那最后就会导致一个山寨的复制品。但如果有一个摸清它规律的过程，最后就有可能会生产出来一个【产品】，这就是差别。

有些中国企业因为成功来得太快，所以不会精益求精(in all sense).

在一个不好的框架下，做出来的东西一定是不好的东西。或许不是模型不对，但整个实现的过程全部都是 bug, 如果一个设计到了最后全部都是补丁，那可能从一开始的设计思路就有问题。但有些时候数据可以弥补模型的不足（但会曲折一点）。

---

语言的出现是为了人类之间的通信。字母（笔画）、文字和数字实际上是信息编码的不同单位。任何一种语言都是一种编码的方式，而语言的语法规则就是编解码的算法。当讲出一句话时，就是用这种语言的编码方式对头脑中的信息做了一次编码，编码的结果就是一串文字。而如果对方懂得这门语言，ta 就可以用这门语言的解码方法获得说话人要表达的信息。这就是语言的数学本质。

如果一个人无法判断自己交流的对象是人还是机器，就说明这个机器有智能了（**图灵测试**）。



### 统计语言模型 Statistical Language Model

基本数学原理：

Markov Chain 马尔科夫链 => bigram model 二元模型 

Law of Large Numbers 大数定理, turn the probability to frequency.

( N-1阶马尔可夫假设 对应的语言模型是 N-gram model), in real world, at most N=3.)

如何解决统计模型的可靠性问题（P=0 happens, 不能忽视，必须解决）？

- 增加数据量，如果增加数据量之后大部分条件概率依然是0，这种模型称之为**不平滑** 
- Good-Turing Estimate: 相信可靠的统计数据，对不可靠的统计数据打折扣的概率估算方法。Use the count of things we've seen once, to help estimate the count of things we've never seen.
- **Zipf定律**可以表述为在自然语言的语料库里，一个单词出现的次数与它在频率表里的排名成反比, 20%的词占了80%的出现频率。

训练模型中语料库的选取也很重要，如果训练语料和模型应用的领域相脱节，那么模型的效果往往会大打折扣。

[Good Turing Smoothing](https://www.youtube.com/watch?v=GwP8gKa-ij8&t=826s)

[The Zipf Mystery](https://www.youtube.com/watch?v=fCn8zs912OE&t=475s)



### 中文分词

一般来讲，应用不同，汉语分词的颗粒度大小应该不同。比如，在机器翻译中，颗粒度应该大一些。在网页搜索中，小的颗粒度就比大的颗粒度要好。**不同的应用应该有不同的分词系统**。

分词在英语中使用场景：在识别手写体时，单词之间的空格不是很清楚，中文分词的方法可以帮助识别英文单词的边界。



### Hidden Markov Model

在从汉语到英语的翻译中，说话者讲的是汉语，但是信道传播编码的方式是英语，如果利用计算机，根据接收到的英语信息，推测说话者的汉语意思，就是**机器翻译**。同样，如果要根据带有拼写错误的与据推测说话者想表达的正确意思，那就是**自动纠错**。这样，几乎所有的自然语言处理问题都可以等价成**通信的解码问题**。

符合马尔可夫假设的随机过程成为马尔可夫过程，也叫马尔可夫链 Markov Chain.

HMM 模型陆续成功的应用于机器翻译、拼写纠错、手写体识别、图像处理、基因序列分析等很多领域。

HMM 也是机器学习的主要工具之一。训练算法： Baum-Welch, 使用时的解码算法： Viterbi algorithm; 掌握这两个算法就可以使用 HMM 这个模型了。

[HMM– Baum Welsh and Viterbi Algorithms](https://www.youtube.com/watch?v=h22nGEF8PUo)

HMM has three problems to deal with:

1. Evaluation - compute the probability of a given observation sequence
2. Decoding - Given an observation sqce, compute the most likely hidden state sequence
3. Learning - Given an observation sequence and set of possible models, which model most closely fits the data?  How do we adjust the model parameters to maximize the p(o|lambda)?

forward prob. : ![](http://latex.codecogs.com/gif.latex?%5Calpha_i%28t%29%20%3D%20P%28o_1....o_t%2Cq_t%20%3D%20s_i%7C%5Clambda%29)





Remark:

本书出现的所有数学公式/理论/证明都在草稿纸上手推一次，工业界怕是再也没有面试能难倒你了（握拳）＼＼\ ٩( 'ω' )و //／／