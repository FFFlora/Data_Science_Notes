### 感知机 perceptron

感知机的几何意义：

![](C:\Users\Flora\Documents\python\data-science-notes\archived-pics\open-course\perceptron.png)

假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的超平面。为了找出这样的超平面S，即感知机模型参数 *w* 和 *b*, 需要确定一个学习策略，即定义损失函数并损失函数极小化。损失函数的一个自然选择是误分类点的总数。但是，这样的损失函数b不是参数 *w* 和 *b* 的连续可导函数，不易优化，所以感知机采用的损失函数为**误分类点到超平面的总距离**。

### Local minimum and global minimum

局部极小，全局最小

通常用以下策略跳出局部极小，进而接近全局最小：

- 以多组不同参数初始化NN，训练后选出最小 loss 的解作为最终参数。
- 模拟退火
- 随机梯度下降
- 遗传算法

爬山算法：兔子朝着比现在高的地方跳去。它找到了不远处的最高山峰。但是这座山不一定是珠穆朗玛峰。这就是爬山算法，它不能保证局部最优值就是全局最优值。

**模拟退火**：兔子喝醉了。它随机地跳了很长时间。这期间，它可能走向高处，也可能踏入平地。但是它渐渐清醒了并朝最高方向跳去。



[试述卷积神经网络的基本原理★](https://tkhajwjy.github.io/2018/11/24/CNN/)